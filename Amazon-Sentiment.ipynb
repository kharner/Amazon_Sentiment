{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Sentiment Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets from Kaggle.com courtesy of Adam Mathias Bittlingmayer. The webpage can be reached through https://www.kaggle.com/bittlingmayer/amazonreviews OR https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.19.1.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'Label', 'Title', 'Description']\n",
    "rawTrainSentiments = pd.read_csv('amazon_review_full_csv/train.csv', names=columns)\n",
    "full_set = rawTrainSentiments.assign(X_train = lambda x: x.Title +' '+ x.Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawTestSentiments = pd.read_csv('amazon_review_full_csv/test.csv', names=columns)\n",
    "full_set_test = rawTestSentiments.assign(X_test = lambda x: x.Title + ' ' + x.Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>X_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>more like funchuck</td>\n",
       "      <td>Gave this to my dad for a gag gift after direc...</td>\n",
       "      <td>more like funchuck Gave this to my dad for a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Inspiring</td>\n",
       "      <td>I hope a lot of people hear this cd. We need m...</td>\n",
       "      <td>Inspiring I hope a lot of people hear this cd....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "      <td>The best soundtrack ever to anything. I'm read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chrono Cross OST</td>\n",
       "      <td>The music of Yasunori Misuda is without questi...</td>\n",
       "      <td>Chrono Cross OST The music of Yasunori Misuda ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Too good to be true</td>\n",
       "      <td>Probably the greatest soundtrack in history! U...</td>\n",
       "      <td>Too good to be true Probably the greatest soun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                  Title  \\\n",
       "0      3                     more like funchuck   \n",
       "1      5                              Inspiring   \n",
       "2      5  The best soundtrack ever to anything.   \n",
       "3      4                       Chrono Cross OST   \n",
       "4      5                    Too good to be true   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Gave this to my dad for a gag gift after direc...   \n",
       "1  I hope a lot of people hear this cd. We need m...   \n",
       "2  I'm reading a lot of reviews saying that this ...   \n",
       "3  The music of Yasunori Misuda is without questi...   \n",
       "4  Probably the greatest soundtrack in history! U...   \n",
       "\n",
       "                                             X_train  \n",
       "0  more like funchuck Gave this to my dad for a g...  \n",
       "1  Inspiring I hope a lot of people hear this cd....  \n",
       "2  The best soundtrack ever to anything. I'm read...  \n",
       "3  Chrono Cross OST The music of Yasunori Misuda ...  \n",
       "4  Too good to be true Probably the greatest soun...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_set['Label'] = full_set['Label'].astype(int)\n",
    "full_set['Title'] = full_set['Title'].astype(str)\n",
    "full_set['Description'] = full_set['Description'].astype(str)\n",
    "full_set['X_train'] = full_set['X_train'].astype(str)\n",
    "\n",
    "full_set_test['Label'] = full_set_test['Label'].astype(int) \n",
    "full_set_test['Title'] = full_set_test['Title'].astype(str)\n",
    "full_set_test['Description'] = full_set_test['Description'].astype(str)\n",
    "\n",
    "full_set_test['X_test'] = full_set_test['X_test'].astype(str)\n",
    "\n",
    "full_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(full_set['Label'])\n",
    "X_train = np.array(full_set['X_train'])\n",
    "\n",
    "y_test = np.array(full_set_test['Label'])\n",
    "X_test = np.array(full_set_test['X_test'])\n",
    "\n",
    "encoded_labels = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "Below I define a contractions dictionary to expand the english contractions.  It is necessary because I will define english, stop words and replace redundant words like 'not'.  The Amazon Sentiment data also includes reviews writen in spanish which is why I have also included Spanish stop words. Finally the word stemmer converts all english verbs into a like tense.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dictionary = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "spanish_stop_words = set(stopwords.words('spanish'))\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    text = text.lower()\n",
    "    regEx = re.compile('(%s)' % '|'.join(contractions_dictionary.keys()))\n",
    "\n",
    "    def expand_contractions(s, contractions_dictionary=contractions_dictionary):\n",
    "        def replace(match):\n",
    "            return contractions_dictionary[match.group(0)]\n",
    "        return regEx.sub(replace, s)\n",
    "    \n",
    "    text = expand_contractions(text, contractions_dictionary)\n",
    "    text = text.split()\n",
    "        \n",
    "    text = [ew for ew in text if not ew in english_stop_words]\n",
    "    text = [sw for sw in text if not sw in spanish_stop_words]\n",
    "    \n",
    "    stem = [stemmer.stem(w) for w in text]\n",
    "    text = \" \".join(stem)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [preprocess_data(text) for text in full_set['X_train']]\n",
    "X_test = [preprocess_data(text) for text in full_set_test['X_test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "I utilized the power of Google's neural network Word2Vec, trained and learned word embeddings as the basis of weights in my own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "MAX_VECTOR_WORDS = 100000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VECTOR_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(X_train + X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "kMAXLEN = 300     #want all comment descriptions to be of size 300 when training\n",
    "kVECTORLEN = 300  #the size of each vector \n",
    "\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "X_train_data = sequence.pad_sequences(X_train_seq, maxlen=kMAXLEN)\n",
    "X_test_data = sequence.pad_sequences(X_test_seq, maxlen=kMAXLEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(tokenizer.word_index)\n",
    "embedded_matrix = np.zeros((nb_words, kVECTORLEN))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec.vocab and i < nb_words:\n",
    "        embedded_matrix[i] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE MODEL\n",
    "After mapping words to corresponding digits and filling the embedded matrix its time to finally train the model.  \n",
    "I use an embedded matrix for the first layer where the weights are borrowed from the genius of Google's Word2Vec.\n",
    "Then I use an convolutional layer to relate words near each other. I also sandwich my LSTM model with dropout layers since they are prone to overfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 300)          293768400 \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 64)           57664     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 300, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150)               109800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 906       \n",
      "=================================================================\n",
      "Total params: 293,942,946\n",
      "Trainable params: 174,546\n",
      "Non-trainable params: 293,768,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "kTOP = vocab_size\n",
    "model = Sequential()\n",
    "model.add( Embedding(kTOP,kVECTORLEN, input_length=kMAXLEN, weights=[embedded_matrix], trainable=False) )\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400000 samples, validate on 600000 samples\n",
      "Epoch 1/1\n",
      "2400000/2400000 [==============================] - 15115s 6ms/step - loss: 1.2657 - acc: 0.4573 - val_loss: 1.1591 - val_acc: 0.5035\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_data, y=encoded_labels, batch_size=32, nb_epoch=1, verbose=1, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy is:  [0.457315]\n"
     ]
    }
   ],
   "source": [
    "print('The model accuracy is: ', history.history['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test_data)\n",
    "test_class_predictions = test_predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.503203076923077\n"
     ]
    }
   ],
   "source": [
    "print('F1 score: ', f1_score(y_test,test_class_predictions,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:  0.503203076923077\n"
     ]
    }
   ],
   "source": [
    "print('Recall score: ', recall_score(y_test,test_class_predictions,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.49580833322182505\n"
     ]
    }
   ],
   "source": [
    "print('Precision: ', precision_score(y_test,test_class_predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[89608 25029  4946  5513  4904]\n",
      " [39673 48423 21441 14309  6154]\n",
      " [13924 27932 40921 36296 10927]\n",
      " [ 5496  8158 16808 60440 39098]\n",
      " [ 5440  3782  3350 29738 87690]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix: \\n', confusion_matrix(y_test, test_class_predictions) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
